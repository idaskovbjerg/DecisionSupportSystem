{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Logistic Regression, LDA, QDA, and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.1 The Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn import neighbors, linear_model, preprocessing\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "1  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "2  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "3  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "4  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "5  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file python\n",
    "df = pd.read_csv('Dataset/Smarket.csv',usecols=range(0,10), index_col=0, parse_dates=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.026155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.030596</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.033195</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.002448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.035689</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.029788</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>-0.034860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>0.030095</td>\n",
       "      <td>-0.026155</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
       "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
       "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
       "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
       "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
       "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
       "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
       "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
       "\n",
       "           Today  \n",
       "Year    0.030095  \n",
       "Lag1   -0.026155  \n",
       "Lag2   -0.010250  \n",
       "Lag3   -0.002448  \n",
       "Lag4   -0.006900  \n",
       "Lag5   -0.034860  \n",
       "Volume  0.014592  \n",
       "Today   1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ida_s\\Anaconda3\\lib\\site-packages\\seaborn\\regression.py:546: UserWarning: The `size` paramter has been renamed to `height`; please update your code.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\ida_s\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1add208fe48>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 324x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lmplot(\"Year\", \"Volume\", data=df, size=4.5)\n",
    "#sns.boxplot(\"Year\",\"Volume\",data=df)\n",
    "# https://towardsdatascience.com/a-guide-to-pandas-and-matplotlib-for-data-exploration-56fad95f951c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the Smarket data is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.formula.glm('Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume', family=sm.families.Binomial(), data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Generalized Linear Model Regression Results                           \n",
      "================================================================================================\n",
      "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                 1250\n",
      "Model:                                              GLM   Df Residuals:                     1243\n",
      "Model Family:                                  Binomial   Df Model:                            6\n",
      "Link Function:                                    logit   Scale:                          1.0000\n",
      "Method:                                            IRLS   Log-Likelihood:                -863.79\n",
      "Date:                                  Sat, 27 Apr 2019   Deviance:                       1727.6\n",
      "Time:                                          22:45:51   Pearson chi2:                 1.25e+03\n",
      "No. Iterations:                                       4   Covariance Type:             nonrobust\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.1260      0.241      0.523      0.601      -0.346       0.598\n",
      "Lag1           0.0731      0.050      1.457      0.145      -0.025       0.171\n",
      "Lag2           0.0423      0.050      0.845      0.398      -0.056       0.140\n",
      "Lag3          -0.0111      0.050     -0.222      0.824      -0.109       0.087\n",
      "Lag4          -0.0094      0.050     -0.187      0.851      -0.107       0.089\n",
      "Lag5          -0.0103      0.050     -0.208      0.835      -0.107       0.087\n",
      "Volume        -0.1354      0.158     -0.855      0.392      -0.446       0.175\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49291587 0.51853212 0.51886117 ... 0.4607317  0.47388171 0.48208344]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict()\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data set is splitted into two - before 2004 and after 2004\n",
      "Before 2004 is used as training set and after 2004 is test.\n",
      "\n",
      "Score of test data\n",
      "0.5595238095238095\n",
      "\n",
      " Predicted  Down   Up  All\n",
      "True                     \n",
      "Down         35   76  111\n",
      "Up           35  106  141\n",
      "All          70  182  252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ida_s\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Splitting in train and test set\n",
    "df_train = df[df.Year < 2005]\n",
    "df_test = df[df.Year == 2005]\n",
    "df_X_train = df_train[:][['Lag1', 'Lag2']]\n",
    "df_X_test = df_test[:][['Lag1', 'Lag2']]\n",
    "df_y_train = df_train[:]['Direction']\n",
    "df_y_test = df_test[:]['Direction']\n",
    "\n",
    "# Fitting the model based on train set\n",
    "lr = linear_model.LogisticRegression()\n",
    "ex = lr.fit(df_X_train, df_y_train)\n",
    "\n",
    "# Print\n",
    "print('\\nData set is splitted into two - before 2004 and after 2004\\nBefore 2004 is used as training set and after 2004 is test.')\n",
    "print('\\nScore of test data')\n",
    "print(lr.score(df_X_test, df_y_test))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n',pd.crosstab(df_y_test, lr.predict(df_X_test), rownames=['True'], colnames=['Predicted'], margins=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the Smarket data set is being used for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/Smarket.csv',usecols=range(1,10), index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[:'2004'][['Lag1','Lag2']]\n",
    "y_train = df[:'2004']['Direction']\n",
    "\n",
    "X_test = df['2005':][['Lag1','Lag2']]\n",
    "y_test = df['2005':]['Direction']\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "pred = lda.fit(X_train, y_train).predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities of Groups:\n",
      " [0.49198397 0.50801603]\n"
     ]
    }
   ],
   "source": [
    "# pi_hat1 and pi_hat2\n",
    "priors = lda.priors_\n",
    "print('Prior probabilities of Groups:\\n', priors)\n",
    "# in other words, 49.2% of the training observations correspond to days during which the market went down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group mean:\n",
      " [[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n"
     ]
    }
   ],
   "source": [
    "m = lda.means_\n",
    "print('Group mean:\\n',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD coefficients:\n",
      " [[-0.64201904]\n",
      " [-0.51352928]]\n"
     ]
    }
   ],
   "source": [
    "# These do not seem to correspond to the values from the R output in the book?\n",
    "coef = lda.scalings_\n",
    "print('LD coefficients:\\n',coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[ 35  35]\n",
      " [ 76 106]]\n"
     ]
    }
   ],
   "source": [
    "# LDA prediction \n",
    "cmatrix = confusion_matrix(y_test, pred).T\n",
    "print('Confusion matrix:\\n',cmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.500     0.315     0.387       111\n",
      "          Up      0.582     0.752     0.656       141\n",
      "\n",
      "   micro avg      0.560     0.560     0.560       252\n",
      "   macro avg      0.541     0.534     0.522       252\n",
      "weighted avg      0.546     0.560     0.538       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([ 70, 182], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50% threshold: allows us to recreate the predictions\n",
    "pred_p = lda.predict_proba(X_test)\n",
    "\n",
    "np.unique(pred_p[:,1]>0.5, return_counts=True)\n",
    "\n",
    "#Notice that the posterior probability output by the model corresponds to the probability that the market will decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False]), array([252], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90% threshold\n",
    "np.unique(pred_p[:,1]>0.9, return_counts=True)\n",
    "\n",
    "# No days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was 52.02%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "# QDA: The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors\n",
    "# Predict works as usually\n",
    "pred = qda.fit(X_train, y_train).predict(X_test)\n",
    "#print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities of Groups:\n",
      " [0.49198397 0.50801603]\n"
     ]
    }
   ],
   "source": [
    "p = qda.priors_\n",
    "print('Prior probabilities of Groups:\\n', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group means:\n",
      " [[ 0.04279022  0.03389409]\n",
      " [-0.03954635 -0.03132544]]\n"
     ]
    }
   ],
   "source": [
    "m = qda.means_\n",
    "print('Group means:\\n',m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[ 30  20]\n",
      " [ 81 121]]\n"
     ]
    }
   ],
   "source": [
    "# This table gives the prediction for 2005 in the confusion matrix\n",
    "cm = confusion_matrix(y_test, pred).T\n",
    "print('Confusion matrix:\\n',cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.600     0.270     0.373       111\n",
      "          Up      0.599     0.858     0.706       141\n",
      "\n",
      "   micro avg      0.599     0.599     0.599       252\n",
      "   macro avg      0.600     0.564     0.539       252\n",
      "weighted avg      0.599     0.599     0.559       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred, digits=3))\n",
    "# Se nederste: dette er den v√¶gtede gennemsnit: mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.5 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[43 58]\n",
      " [68 83]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.426     0.387     0.406       111\n",
      "          Up      0.550     0.589     0.568       141\n",
      "\n",
      "   micro avg      0.500     0.500     0.500       252\n",
      "   macro avg      0.488     0.488     0.487       252\n",
      "weighted avg      0.495     0.500     0.497       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K = 1\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "print('Confusion matrix:\\n',confusion_matrix(y_test, pred).T)\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, pred, digits=3))\n",
    "# K = 1 are not very good, since only 50% of the observations are correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[48 55]\n",
      " [63 86]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.466     0.432     0.449       111\n",
      "          Up      0.577     0.610     0.593       141\n",
      "\n",
      "   micro avg      0.532     0.532     0.532       252\n",
      "   macro avg      0.522     0.521     0.521       252\n",
      "weighted avg      0.528     0.532     0.529       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K = 3\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "print('Confusion matrix:\\n',confusion_matrix(y_test, pred).T)\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results have improved slightly. But increasing K further turns out to provide no further improvements. \n",
    "It appears that for this data, QDA provides the best results of the methods that we have examined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
