{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Logistic Regression, LDA, QDA, and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.1 The Stock Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn import neighbors, linear_model, preprocessing\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn import datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>5.010</td>\n",
       "      <td>1.1913</td>\n",
       "      <td>0.959</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>-1.055</td>\n",
       "      <td>1.2965</td>\n",
       "      <td>1.032</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>-2.624</td>\n",
       "      <td>1.4112</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>-0.192</td>\n",
       "      <td>1.2760</td>\n",
       "      <td>0.614</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.614</td>\n",
       "      <td>-0.623</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.381</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>0.213</td>\n",
       "      <td>Up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year   Lag1   Lag2   Lag3   Lag4   Lag5  Volume  Today Direction\n",
       "1  2001  0.381 -0.192 -2.624 -1.055  5.010  1.1913  0.959        Up\n",
       "2  2001  0.959  0.381 -0.192 -2.624 -1.055  1.2965  1.032        Up\n",
       "3  2001  1.032  0.959  0.381 -0.192 -2.624  1.4112 -0.623      Down\n",
       "4  2001 -0.623  1.032  0.959  0.381 -0.192  1.2760  0.614        Up\n",
       "5  2001  0.614 -0.623  1.032  0.959  0.381  1.2057  0.213        Up"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file python\n",
    "df = pd.read_csv('Dataset/Smarket.csv',usecols=range(0,10), index_col=0, parse_dates=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Lag1</th>\n",
       "      <th>Lag2</th>\n",
       "      <th>Lag3</th>\n",
       "      <th>Lag4</th>\n",
       "      <th>Lag5</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.030596</td>\n",
       "      <td>0.033195</td>\n",
       "      <td>0.035689</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.030095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>0.029700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.026155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.030596</td>\n",
       "      <td>-0.026294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.010250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag3</th>\n",
       "      <td>0.033195</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.002448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag4</th>\n",
       "      <td>0.035689</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.024051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag5</th>\n",
       "      <td>0.029788</td>\n",
       "      <td>-0.005675</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>-0.027084</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>-0.034860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>0.539006</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>-0.043383</td>\n",
       "      <td>-0.041824</td>\n",
       "      <td>-0.048414</td>\n",
       "      <td>-0.022002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today</th>\n",
       "      <td>0.030095</td>\n",
       "      <td>-0.026155</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>-0.002448</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.034860</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year      Lag1      Lag2      Lag3      Lag4      Lag5    Volume  \\\n",
       "Year    1.000000  0.029700  0.030596  0.033195  0.035689  0.029788  0.539006   \n",
       "Lag1    0.029700  1.000000 -0.026294 -0.010803 -0.002986 -0.005675  0.040910   \n",
       "Lag2    0.030596 -0.026294  1.000000 -0.025897 -0.010854 -0.003558 -0.043383   \n",
       "Lag3    0.033195 -0.010803 -0.025897  1.000000 -0.024051 -0.018808 -0.041824   \n",
       "Lag4    0.035689 -0.002986 -0.010854 -0.024051  1.000000 -0.027084 -0.048414   \n",
       "Lag5    0.029788 -0.005675 -0.003558 -0.018808 -0.027084  1.000000 -0.022002   \n",
       "Volume  0.539006  0.040910 -0.043383 -0.041824 -0.048414 -0.022002  1.000000   \n",
       "Today   0.030095 -0.026155 -0.010250 -0.002448 -0.006900 -0.034860  0.014592   \n",
       "\n",
       "           Today  \n",
       "Year    0.030095  \n",
       "Lag1   -0.026155  \n",
       "Lag2   -0.010250  \n",
       "Lag3   -0.002448  \n",
       "Lag4   -0.006900  \n",
       "Lag5   -0.034860  \n",
       "Volume  0.014592  \n",
       "Today   1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ida_s\\Anaconda3\\lib\\site-packages\\seaborn\\regression.py:546: UserWarning: The `size` paramter has been renamed to `height`; please update your code.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\ida_s\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x2418658e198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 324x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lmplot(\"Year\", \"Volume\", data=df, size=4.5)\n",
    "#sns.boxplot(\"Year\",\"Volume\",data=df)\n",
    "# https://towardsdatascience.com/a-guide-to-pandas-and-matplotlib-for-data-exploration-56fad95f951c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the Smarket data is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.formula.glm('Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume', family=sm.families.Binomial(), data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Generalized Linear Model Regression Results                           \n",
      "================================================================================================\n",
      "Dep. Variable:     ['Direction[Down]', 'Direction[Up]']   No. Observations:                 1250\n",
      "Model:                                              GLM   Df Residuals:                     1243\n",
      "Model Family:                                  Binomial   Df Model:                            6\n",
      "Link Function:                                    logit   Scale:                          1.0000\n",
      "Method:                                            IRLS   Log-Likelihood:                -863.79\n",
      "Date:                                  Fri, 26 Apr 2019   Deviance:                       1727.6\n",
      "Time:                                          11:10:15   Pearson chi2:                 1.25e+03\n",
      "No. Iterations:                                       4   Covariance Type:             nonrobust\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.1260      0.241      0.523      0.601      -0.346       0.598\n",
      "Lag1           0.0731      0.050      1.457      0.145      -0.025       0.171\n",
      "Lag2           0.0423      0.050      0.845      0.398      -0.056       0.140\n",
      "Lag3          -0.0111      0.050     -0.222      0.824      -0.109       0.087\n",
      "Lag4          -0.0094      0.050     -0.187      0.851      -0.107       0.089\n",
      "Lag5          -0.0103      0.050     -0.208      0.835      -0.107       0.087\n",
      "Volume        -0.1354      0.158     -0.855      0.392      -0.446       0.175\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49291587 0.51853212 0.51886117 ... 0.4607317  0.47388171 0.48208344]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict()\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data set is splitted into two - before 2004 and after 2004\n",
      "Before 2004 is used as training set and after 2004 is test.\n",
      "\n",
      "Score of test data\n",
      "0.5595238095238095\n",
      "\n",
      " Predicted  Down   Up  All\n",
      "True                     \n",
      "Down         35   76  111\n",
      "Up           35  106  141\n",
      "All          70  182  252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ida_s\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Splitting in train and test set\n",
    "df_train = df[df.Year < 2005]\n",
    "df_test = df[df.Year == 2005]\n",
    "df_X_train = df_train[:][['Lag1', 'Lag2']]\n",
    "df_X_test = df_test[:][['Lag1', 'Lag2']]\n",
    "df_y_train = df_train[:]['Direction']\n",
    "df_y_test = df_test[:]['Direction']\n",
    "\n",
    "# Fitting the model based on train set\n",
    "lr = linear_model.LogisticRegression()\n",
    "ex = lr.fit(df_X_train, df_y_train)\n",
    "print('\\nData set is splitted into two - before 2004 and after 2004\\nBefore 2004 is used as training set and after 2004 is test.')\n",
    "print('\\nScore of test data')\n",
    "print(lr.score(df_X_test, df_y_test))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n',pd.crosstab(df_y_test, lr.predict(df_X_test), rownames=['True'], colnames=['Predicted'], margins=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.3 Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the Smarket data set is being used for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/Smarket.csv',usecols=range(1,10), index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[:'2004'][['Lag1','Lag2']]\n",
    "y_train = df[:'2004']['Direction']\n",
    "\n",
    "X_test = df['2005':][['Lag1','Lag2']]\n",
    "y_test = df['2005':]['Direction']\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "pred = lda.fit(X_train, y_train).predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49198397, 0.50801603])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pi_hat1 og pi_hat2\n",
    "lda.priors_\n",
    "# in other words, 49.2% of the training observations correspond to days during which the market went down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04279022,  0.03389409],\n",
       "       [-0.03954635, -0.03132544]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05544078, -0.0443452 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These do not seem to correspond to the values from the R output in the book?\n",
    "lda.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 35,  35],\n",
       "       [ 76, 106]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA prediction \n",
    "confusion_matrix(y_test, pred).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.500     0.315     0.387       111\n",
      "          Up      0.582     0.752     0.656       141\n",
      "\n",
      "   micro avg      0.560     0.560     0.560       252\n",
      "   macro avg      0.541     0.534     0.522       252\n",
      "weighted avg      0.546     0.560     0.538       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([ 70, 182], dtype=int64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 50% threshold: allows us to recreate the predictions\n",
    "pred_p = lda.predict_proba(X_test)\n",
    "\n",
    "np.unique(pred_p[:,1]>0.5, return_counts=True)\n",
    "\n",
    "#Notice that the posterior probability output by the model corresponds to the probability that the market will decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False]), array([252], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90% threshold\n",
    "np.unique(pred_p[:,1]>0.9, return_counts=True)\n",
    "\n",
    "# No days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was 52.02%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.4 Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "# QDA: The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors\n",
    "# Predict works as usually\n",
    "pred = qda.fit(X_train, y_train).predict(X_test)\n",
    "#print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49198397, 0.50801603])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda.priors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04279022,  0.03389409],\n",
       "       [-0.03954635, -0.03132544]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30,  20],\n",
       "       [ 81, 121]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This table gives the prediction for 2005 in the confusion matrix\n",
    "confusion_matrix(y_test, pred).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.600     0.270     0.373       111\n",
      "          Up      0.599     0.858     0.706       141\n",
      "\n",
      "   micro avg      0.599     0.599     0.599       252\n",
      "   macro avg      0.600     0.564     0.539       252\n",
      "weighted avg      0.599     0.599     0.559       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred, digits=3))\n",
    "# Se nederste: dette er den vægtede gennemsnit: mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6.5 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43 58]\n",
      " [68 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.426     0.387     0.406       111\n",
      "          Up      0.550     0.589     0.568       141\n",
      "\n",
      "   micro avg      0.500     0.500     0.500       252\n",
      "   macro avg      0.488     0.488     0.487       252\n",
      "weighted avg      0.495     0.500     0.497       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K = 1\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "print(confusion_matrix(y_test, pred).T)\n",
    "print(classification_report(y_test, pred, digits=3))\n",
    "# K = 1 are not very good, since only 50% of the observations are correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48 55]\n",
      " [63 86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down      0.466     0.432     0.449       111\n",
      "          Up      0.577     0.610     0.593       141\n",
      "\n",
      "   micro avg      0.532     0.532     0.532       252\n",
      "   macro avg      0.522     0.521     0.521       252\n",
      "weighted avg      0.528     0.532     0.529       252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K = 3\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "print(confusion_matrix(y_test, pred).T)\n",
    "print(classification_report(y_test, pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results have improved slightly. But increasing K further turns out to provide no further improvements. \n",
    "It appears that for this data, QDA provides the best results of the methods that we have examined so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
